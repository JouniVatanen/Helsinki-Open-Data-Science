```{r, include = F}
knitr::opts_chunk$set(echo = F, comment = NA)
```

Logistic regression and cross-validation
========================================================
css: index.css

For IODS by Tuomo Nieminen

<br>
<br>
<br>
<br>

Powered by RMarkdown. The code for this presentation is  [here.](https://github.com/TuomoNieminen/Helsinki-Open-Data-Science/blob/master/docs/logistic_regression.Rpres)


Logistic regression
========================================================
type: prompt
incremental: false

Odds and probability
```{r}
p <- seq(0,0.96,by=0.01)
df <- data.frame(p = p, odds = p / (1-p))
library(ggplot2)
ggplot(df, aes(x = p, y = odds)) + geom_line(col = "salmon", size = 1.1)
```

***

Predicting binary outcomes
```{r}
set.seed(333)
n <- 50
x <- rnorm(n)
p <- 1 / (1 + exp(-2*x))
y <- rbinom(n, 1, p)
m <- glm(y~x, family = "binomial")
df <- data.frame(x = x, y = y, p = predict(m, type = "response"))
df <- df[order(df$x),]
library(ggplot2)
q <- ggplot(df, aes(x = x, y = p))
q <- q + geom_line(color = "blue", size = 1.1)
q <- q + geom_point(aes(y = y), size = 4, alpha = 0.3)
q + ggtitle("")
```


A conditional look at regression
========================================================

In regression analysis, the target variable $\boldsymbol{Y}$ is modelled as a linear combination of model parameters and the explanatory variables $\boldsymbol{X}$

$$\boldsymbol{Y} = \boldsymbol{\alpha} + \boldsymbol{\beta}\boldsymbol{X} + \boldsymbol{\epsilon}$$

Another way to express this is to use conditional expectation

$$E[\boldsymbol{Y} \mid \boldsymbol{X}] = \boldsymbol{\alpha} + \boldsymbol{\beta}\boldsymbol{X}$$

So, linear regression is a model for the (conditional) expected value of Y.


Regression for binary outcomes
========================================================

If the target variable Y is binary, taking only the values 0 or 1 with probability $p$, then

$$E[Y] = p$$

- The goal in logistic regression is to define a linear model for the probability of success (Y = 1) $p$.
- The problem is that $p$ only takes on values between 0 and 1

Luckily, there is a trick


The logit function
========================================================
incremental: false

We can apply the *logit function* to the expected value of Y ($p$).

$$log\frac{p}{1 - p} \in [- \infty, \infty]$$

- After the transformation, the possible values are all the real numbers.

***

```{r}
logit <- function(p) log(p/(1-p))
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p", 
      ylim = c(-5, 5), n = 300)
abline(h = 0, lty = 2, col = "grey70")
```

That's odds
========================================================
One reason to use the logit transformation is that is has a nice interpretation.

- The part $p/1-p$ of the logit transformation are called the odds: the ratio of successes to failures
- Higher odds corresponds to a higher probability of success

***

```{r}
p <- seq(0,0.96,by=0.01)
df <- data.frame(p = p, odds = p / (1-p))
library(ggplot2)
ggplot(df, aes(x = p, y = odds)) + geom_line(col = "blue", size = 1.1) + ggtitle("Odds and probability")
```


Odds ratio
========================================================
The ratio of two odds is called the odds ratio.


Intepreting the parameters of logistic regression
========================================================
Example of how to intepret the parameters of logistic regression as odds ratios


Cross-validation
========================================================
type: prompt
