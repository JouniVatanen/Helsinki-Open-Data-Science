Dimensionality Reduction Techniques
========================================================
type: sub-section

For IODS by Tuomo Nieminen

<br>
<br>
<br>
<br>

Powered by RMarkdown. The code for this presentation is  [here.](https://github.com/TuomoNieminen/Helsinki-Open-Data-Science/blob/master/docs/dimensionality_reduction.Rpres)

```{r, include = F}
# setup
knitr::opts_chunk$set(echo = F, comment = NA)
mytheme <- function(axt_text = 18, axt_title = 22 ) {
  ggplot2::theme(axis.text=element_text(size = axt_text),
                 axis.title=element_text(size = axt_title, face="bold"))
}
```


Pricipal component analysis
========================================================
type: prompt
incremental: false

```{r}
# 3d data
set.seed(333)
x <- rep(c(1,2,5), times = 50)
col <- x
x <- jitter(x, factor = 2)
X <- matrix(c(x, x + rnorm(length(x)), x + rnorm(length(x))), ncol = 3)
```

From high...
```{r}
# plot original 3d data
library(scatterplot3d)
scatterplot3d(X, 
              cex.symbols = 2.0, pch = 20, box = F,
              color = col + 1,
              xlab = "X1", ylab = "X2", zlab = "X3")
```

***

.. to lower dimensionality
```{r}

par(mar = c(4,4,3,1))

pca <- prcomp(X, retx = F, center = T, scale = T)
newdata <- X %*% pca$rotation
eigenv <- pca$sdev**2
var_explained <- paste0("(",round(100*(eigenv/sum(eigenv)), 1),"%)")
colnames(newdata) <- paste(colnames(newdata), var_explained)

plot(newdata[,1:2], ylim = c(-5, 5), 
     bty = "n",
     cex = 2.0, pch = 20, col = col + 1)
```

What is dimensionality?
========================================================

In statistical analysis, one can think of *dimensionality* as the number of variables (features) related to each observation in the data.

- If each observation is measured by $d$ number of features, then the data is $d$ dimensional. Each observation needs $d$ points to define it's location in a mathematical space.
- If there are a lot of features, some of them can relate to the same underlying dimensions (not directly measured)
- Some dimensions may be stronger and some weaker, they are not equally important

Dimensionality reduction
========================================================
  
The original variables of high dimensional data might contain "too much" information (and noise or some other random error) for representing the underlying phenomenom of interest

- A solution is to reduce the number of dimensions and focus only on the most essential dimensions extracted from the data
- In practise we can *transform* the data and use only a few *principal components* for visualisation and/or analysis
- Hope is that the variance along a small number of principal components provides a reasonable characterization of the complete data set

Tools for dimensionality reduction
========================================================

On the linear algebra level, Singular Value Decomposition (SVD) is the most important tool for reducing the number of dimensions in multivariate data.

- Principal Component Analysis (PCA) is an important statistical procedure which does the same thing
- LDA can also be considered as a dimensionality reduction technique. The visualisation techniques related to PCA and LDA are similar (biplot).
- Unlike LDA, PCA has no criteria or target variable for the reduction. In machine learning vocabulary, PCA may therefore be called an *unsupervised* method.


Pricipal component analysis (PCA) (1)
========================================================

In PCA, the data is first *projected* to a new space with the same number of dimensions (new features). These new features always have the following properties:

- The first dimension (1st principal component) captures the maximum amount of variance from the features in the original data.
- The second dimension (2nd principal component) is orthogonal to the first (they are uncorrelated) and captures the maximum amount of variability left.
- The same is true for each principal component. They are all orthogonal and each is "less important" than the previous one, in terms of captured variability.

Principal component analysis (PCA) (2)
========================================================

Given the properties of the principal components, we can simply **choose the first few principal components** to represent our data.

This will give us uncorrelated variables which capture the maximum amount of variation in the data!

***

```{r, fig.height = 5}
library(ggfortify)
library(dplyr)
pc <- prcomp(iris[-5])
vars <- pc$sdev**2
varcap <- (100*vars / (sum(vars))) %>% round(2)
lab <- paste0(colnames(pc$rotation), " (", varcap, "%)")
autoplot(pc, loadings = T, data = iris, col = "Species", loadings.label.size = 5,
         loadings.label = T, loadings.label.vjust = 1.5, xlab = lab[1], ylab = lab[2],
         main = "A biplot of iris's 2 principal components") + mytheme()
```

<small>*The dimensionality of iris reduced to two principal components (PC). The first PC captures more than 90% of the total variance in the 4 original variables.*</small>

About PCA
========================================================
PCA is a mathematical tool, not a statistical model, which is why linear algebra (SVD) is enough.

- PCA is most powerful at encapsulating linear relationships (correlations)
- There is no statistical model for separating the sources of variance. All variance is thought to be from the same - although multidimensional - source.
- It is also possible to model the dimensionality using underlying latent variables with for example Factor Analysis
- These advanced methods of multivariate analysis are not part of this course
