```{r, include = F}
knitr::opts_chunk$set(echo = F, comment = NA)
```


```{r, include = F}
mytheme <- function(axt_text = 18, axt_title = 22 ) {
  ggplot2::theme(axis.text=element_text(size = axt_text),
        axis.title=element_text(size = axt_title, face="bold"))
}
```

Clustering and classification
========================================================
css: index.css
type: sub-section

For IODS by Emma Kämäräinen

<br>
<br>
<br>
<br>

Powered by RMarkdown. The code for this presentation is  [here.](https://github.com/TuomoNieminen/Helsinki-Open-Data-Science/blob/master/docs/cluster_classification.Rpres)

Clustering and classification
========================================================
incremental: false

Classification:
- You know the number of classes
- The classification model is trained based on data
- Classify new observation

***

Clustering: 
- Unknown classes / number of classes is unknown
- Find groups within data based on similarity 

Clustering and classification
========================================================
incremental: false

- Linear discriminant analysis
- Distance measures
- K-means

Linear discriminant analysis
==================================================

Linear discriminant analysis (LDA) is a classification method. It can be used to model binary variables, like in logistic regression, or multiple class variables. The target variable needs to be categorical. 

It can be used to
- Find the variables that discriminate/separate the classes best
- Prediction of classes
- Dimension reduction

Linear discriminant analysis
==================================================

Assumptions:
- Data variables are normally distributed
- Each variable has the same variance
  - Because of the variance assumption, the data might need scaling before fitting the model 

Linear discriminant analysis
==================================================

Classifying new observations:
- Based on the trained model LDA calculates the probabilities for the new observation for belonging in each of the classes
- The observation is classified to the class of the highest probability
- The math behind the probabilities can be seen [here](http://scikit-learn.org/stable/modules/lda_qda.html) for those who are interested. [Bayes theorem](https://en.wikipedia.org/wiki/Bayes'_theorem) is used to estimate the probabilities.


Distance measures
==================================================
incremental: false

How to determine if observations are similar or dissimilar with each others?

- [Euklidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)
- [Manhattan/Taxicab distance](https://en.wikipedia.org/wiki/Taxicab_geometry) (axis-aligned directions)
- [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index) (binary/categorical distance measure)
- [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance) (distance measure for words/strings)

K -means
==================================================
incremental: false
autosize: true

- *K*-means is possibly the oldest and used clustering method in many fields of study
- Pro: Easy to use and often finds a solution
- Con: Small change in the dataset can produce very different results
- Many variations of *k*-means: *k*-means++, *k*-medoids, *k*-medians... 
    
K-means algorithm
==================================================
incremental: false
autosize: true

1. Choose the number of clusters you want to have and pick initial cluster centroids.
2. Calculate distances between centroids and datapoints. 
3. For all the data points: Assign data point to cluster based on which centroid is closest.
4. Update centroids: within each cluster, calculate new centroid
5. Update clusters: Calculate distances between data points and updated centroids. If some other centroid is closer than the cluster centroid where the data point belongs, the data point changes cluster.

Continue updating steps until the centroids or the clusters do not change

K-means example
==================================================

```{r, echo=F}
data(iris)
library(ggplot2)
g <- ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) 
g + geom_point(size = 4) + mytheme()

```

***

```{r, echo=F}
set.seed(20)
km <- kmeans(iris[,1:4],3)
cluster<- as.factor(km$cluster)
iris <- data.frame(iris, cluster)
g <- ggplot(iris, aes(Sepal.Length, Sepal.Width, color = cluster)) 
g + geom_point(size = 4) + mytheme()
```

Source: [This R-Blogges Post](https://www.r-bloggers.com/k-means-clustering-in-r/)
 
K-means notes
==================================================
incremental: false
autosize: true

Remarks about *k*-means:
- Distance measure in the algorithm: similarity/dissimilarity measure between data points
  - Different distance measures produce different output
  - Deciding the best distance is not
- Number of clusters as input
  - Many ways to find the optimal number of clusters
  - One way is to look at the total within cluster sum of squares (see next slide)
  - [Other ways](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set): hierarchical clustering, silhouette method, cross validation ...

K-means: Total within sum of squares
==================================================
incremental: false
autosize: true

Total within sum of squares is calculated by adding the within cluster sum of squares (WCSS) of every cluster together. The WCSS can be calculated with the pattern 

$WCSS = \sum_i^N (x_i - centroid)^2$

So you are searching for the number of clusters, where the observations are closest to the cluster center. 
