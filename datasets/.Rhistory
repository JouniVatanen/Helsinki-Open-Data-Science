5+5
install.packages("knitr")
install.packages("ggplot2")
install.packages("scatter3d")
install.packages("scatterplot3d")
install.packages("ggfortify")
library(ggfortify)
?autoplot
lm_fit <- lm(attitude~points, data = learning2014)
learning2014 <-  read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/learning2014.txt", sep = "\t", header = TRUE)
learning2014 <- learning2014[learning2014$points > 0,]
lm_fit <- lm(attitude~points, data = learning2014)
lm_fit <- lm(attitude~points, data = learning2014)
autoplot(lm_fit, which = c(1,2,5), smooth.linetype = "none")
lm_fit <- lm(attitude~points, data = learning2014)
autoplot(lm_fit, which = c(1,2,5), smooth.linetype = NULL)
lm_fit <- lm(attitude~points, data = learning2014)
autoplot(lm_fit, which = c(1,2,5), smooth.linetype = "blank")
?geom_smooth
install.packages("broom")
?logit
curve(function(x) log(x/(1-x)))
curve(expression(function(x) log(x/(1-x))))
logit <- function(x) log(x/(1-x))
curve(logit)
`log`
?log
exp(1)
curve(logit, mian = "logit of p")
curve(logit, main = "logit of p")
curve(logit, main = "logit of p")
logit <- function(p) log(p/(1-p))
curve(logit, main = "logit of p")
?curve
curve(logit, xname = "p", main = "logit of p")
curve(logit, xname = "p", col = "blue", main = "logit of p")
curve(logit, xname = "p", col = "green", main = "logit of p")
curve(logit, xname = "p", col = "green", lwd = 2, main = "logit of p")
curve(logit, xname = "p", col = "green", lwd = 1.5, main = "logit of p")
curve(logit, xname = "p", col = "blue", lwd = 1.5, main = "logit of p")
curve(logit, xname = "p", col = "blue", lwd = 1.8, main = "logit of p")
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p")
curve(logit, xname = "p", col = "blue", lwd = 1, main = "logit of p")
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p")
abline(h = 0)
abline(h = 0, tly = 2)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p")
abline(h = 0, lty = 2)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p")
abline(h = 0, lty = 2, col = "grey80")
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p")
abline(h = 0, lty = 2, col = "grey70")
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-6, 6))
abline(h = 0, lty = 2, col = "grey70")
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-6, 6), from = 0, to = 1)
?curve
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-6, 6), from = 0, to = 1, n = 1000)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-6, 6), from = 0, to = 1, n = 500)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-6, 6), n = 500)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-6, 6), n = 200)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-6, 6), n = 200)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-6, 6), n = 300)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-20, 20), n = 300)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-4, 4), n = 300)
curve(logit, xname = "p", col = "blue", lwd = 2, main = "logit of p",
ylim = c(-5, 5), n = 300)
abline(h = 0, lty = 2, col = "grey70")
p <- seq(0,1,by=0.01)
odds <- p/(1-p)
plot(p, odds)
plot(p, odds, type = "l")
plot(p, odds, type = "l", col = "blue", lwd = 2)
plot(p, odds, type = "l", col = "blue", lwd = 2, ylim = c(0, 10))
plot(p, odds, type = "l", col = "blue", lwd = 2, ylim = c(0, 20))
plot(p, odds, type = "l", col = "blue", lwd = 2, ylim = c(0, 10))
plot(p, odds, type = "l", col = "blue", lwd = 2, ylim = c(0, 20))
ggplot2::qplot(p, odds, geom = "line", col = "blue", ylim = c(0, 20))
ggplot2::qplot(p, odds, geom = "line", ylim = c(0, 20))
library(ggplot2)
ggplot2::qplot(p, odds, ylim = c(0, 20)) + geom_line()
ggplot2::qplot(p, odds, ylim = c(0, 20))
ggplot2::qplot(p, odds)
p <- seq(0,1,by=0.05)
odds <- p/(1-p)
library(ggplot2)
ggplot2::qplot(p, odds)
p <- seq(0,0.99,by=0.01)
odds <- p/(1-p)
library(ggplot2)
ggplot2::qplot(p, odds)
p <- seq(0,0.97,by=0.01)
odds <- p/(1-p)
library(ggplot2)
ggplot2::qplot(p, odds)
p <- seq(0,0.96,by=0.01)
odds <- p/(1-p)
library(ggplot2)
ggplot2::qplot(p, odds)
ggplot2::qplot(p, odds) + geom_line(col = "blue")
ggplot(cbind(p, odds), aes(x = p, y = odds) + geom_line(col = "blue")
```
cbind(p, odds)
ggplot(cbind(p, odds), aes(x = p, y = odds)) + geom_line(col = "blue")
data.frame(p = seq(0,0.96,by=0.01),odds = p/(1-p)
library(ggplot2)
ggplot(cbind(p, odds), aes(x = p, y = odds)) + geom_line(col = "blue")
```
data.frame(p = seq(0,0.96,by=0.01), odds = p/(1-p))
df <- data.frame(p = seq(0,0.96,by=0.01), odds = p/(1-p))
ggplot(df, aes(x = p, y = odds)) + geom_line(col = "blue")
ggplot(df, aes(x = p, y = odds)) + geom_line(col = "blue", size = 2)
ggplot(df, aes(x = p, y = odds)) + geom_line(col = "blue", size = 1.5)
ggplot(df, aes(x = p, y = odds)) + geom_line(col = "blue", size = 1.1)
ggplot(df, aes(x = p, y = odds)) + geom_line(col = "blue", size = 1.1) + ggtitle("Odds and probability")
n <- 50
x <- rnorm(n)
p <- 1/(1 + exp(-x))
y <- rbinom(n, p)
?rbinom
y <- rbinom(n, 1, p)
y
m <- glm(y~x, family = "binomial")
plot(m)
plot(m)
df <- data.frame(y = y, p = predict(m))
df <- data.frame(y = y, p = predict(m, type = "response"))
df
plot(x, y)
lines(p, data = df)
?lines
lines(p ~ x, data = df)
plot(x, y)
plot(x, y)
df <- data.frame(x = x, y = y, p = predict(m, type = "response"))
lines(p ~ x, df)
plot(x, y)
lines(x ~ p, df)
df <- df[sort(df$x),]
df <- df[sort(df$x),]
df <- df[order(df$x),]
plot(x, y)
lines(x ~ p, df)
plot(x, y)
lines(p ~ x, df)
plot(x, y)
lines(p ~ x, df, col = "blue", lwd = 2)
plot(x, y, lty = 20)
?par
plot(x, y, pch = 20)
lines(p ~ x, df, col = "blue", lwd = 2)
q <- ggplot(df, aes(x = x, y = y))
q + geom_point()
q + geom_line(aes(x = x, y = p))
q <- ggplot(df, aes(x = x, y = p))
q <- q + geom_point(y = y)
q <- ggplot(df, aes(x = x, y = p))
q <- q + geom_point(y = y)
q
q + geom_line(y = p)
q <- ggplot(df, aes(x = x, y = p))
q <- q + geom_point(y = y)
q + geom_line(x = x, y = p)
q <- ggplot(df, aes(x = x, y = y))
q <- q + geom_point()
q
q + geom_line(x = x, y = p)
q <- ggplot(df, aes(x = x, y = y))
q <- q + geom_point()
q + geom_line(y = p)
q <- ggplot(df, aes(x = x, y = p))
q <- q + geom_linet()
q
q <- ggplot(df, aes(x = x, y = p))
q <- q + geom_line()
q
q + geom_poin(y)
q + geom_point(y)
q + geom_point(aes(y = y))
q <- ggplot(df, aes(x = x, y = p))
q <- q + geom_line(color = "blue")
q + geom_point(aes(y = y))
q <- ggplot(df, aes(x = x, y = p))
q <- q + geom_line(color = "blue", size = 1.1)
q + geom_point(aes(y = y))
127 / 1000
4.5/35
33350 / 127000
1/4.5
4.5*33351
4.5*33351
4*33351
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
lrn14$attitude <- lrn14$Attitude / 10
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D07","D14","D22","D30")
lrn14$deep <- rowMeans(lrn14[, deep_questions])
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
lrn14$surf <- rowMeans(lrn14[, surface_questions])
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
lrn14$stra <- rowMeans(lrn14[, strategic_questions])
learning2014 <- lrn14[, c("gender","Age","attitude", "deep", "stra", "surf", "Points")]
colnames(learning2014)[2] <- "age"
colnames(learning2014)[7] <- "points"
learning2014 <- learning2014[learning2014$points > 0, ]
dim(learning2014)
x<-rnorm(20);y<-x+rnorm(20);summary(lm(x~y))
x<-rnorm(20);y<-x+rnorm(20);m <- lm(x~y); summary(m)
plot(m)
knitr::opts_chunk$set(echo = TRUE)
plot(m, which = c(1,2,5))
x<-rnorm(20);y<-x+rnorm(20);m <- lm(x~y);
plot(m, which = c(1,2,5))
x<-rnorm(20);y<-x+rnorm(20);m <- lm(x~y);
par(mfrow = c(2,2))
plot(m, which = c(1,2,5))
?predict
learning2014 <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/learning2014.txt", sep = ",", header = T)
my_fit <- lm(points ~ attitude, data = learning2014)
new_attitudes <- c("Mia" = 3.8, "Mike"= 4.4, "Riikka" = 2.2, "Pekka" = 2.9)
new_data <- data.frame(attitude = new_attitudes)
new_data
new_data
predict(my_fit, newdata = new_data)
summary(my_fit)
?summary
?as.table
as.table(c(2,2))
as.table(c(2,2,2,2))
as.table(data.frame(E = c(4,9), O = c(4,9))
)
as.table(data.frame(E = c(4,9), O = c(4,9)))
data.frame(E = c(4, 9), O = c(4, 9))
addmargins(data.frame(E = c(4, 9), O = c(4, 9)))
as.table(matrix(E = c(4, 9), O = c(4, 9)))
as.table(matrix(c(4, 9, 3, 7)), ncol = 2)
matrix(c(4, 9, 3, 7)), ncol = 2)
matrix(c(4, 9, 3, 7), ncol = 2)
as.table(matrix(c(4, 9, 3, 7), ncol = 2))
as.table(matrix(c(4, 9, 3, 7), ncol = 2, colnames = c("E", "O")))
as.table(matrix(c(4, 9, 3, 7), ncol = 2, col.names = c("E", "O")))
as.table(matrix(c(4, 9, 3, 7), ncol = 2))
as.table(matrix(c(10, 14, 21, 7), ncol = 2))
as.table(matrix(c(10, 14, 21, 7), ncol = 2), dnn = c("E", "E-", "O", "O-")
```
as.table(matrix(c(10, 14, 21, 7), ncol = 2), dnn = list(c("E", "E-"), c("O", "O-")))
E-=2
E = 10;E_= 14;O=21;O_=7
as.table(E, E_, O, O_)
table(E, E_, O, O_)
E <-  list("E" = 10, "E-" = 14)
O <- list("O" = 21, "O-" = 7)
table(E, O)
as.table(E, O)
E <-  c("E" = 10, "E-" = 14)
O <- c("O" = 21, "O-" = 7)
data.frame(E, O)
E <-  c("E" = 10, "E-" = 14)
O <- c("O" = 21, "O-" = 7)
data.frame(E, O)
data.frame(e = E, O = O)
data.frame("O" = c("E" = 10, "E-" = 14), "O-" = c("E" = 21, "E-" = 7))
data.frame("O" = c("E" = 10, "E-" = 14), `O-` = c("E" = 21, "E-" = 7))
data.frame("O" = c("E" = 10, "E_" = 14), "O_" = c("E" = 21, "E_" = 7))
data.frame("E" = c("O" = 10, "O_" = 14), "E_" = c("O" = 21, "O_" = 7))
d <- data.frame("E" = c("O" = 10, "O_" = 14), "E_" = c("O" = 21, "O_" = 7))
d$total <- rowSums(d)
rbind(d, "total" = colSums(d))
d <- data.frame("E" = c("O" = 10, "O_" = 14), "E_" = c("O" = 21, "O_" = 7))
d$t <- rowSums(d)
rbind(d, "t" = colSums(d))
d <- data.frame("E" = c("O" = 10, "O_" = 14), "E_" = c("O" = 21, "O_" = 7))
d$tot <- rowSums(d)
rbind(d, "tot" = colSums(d))
d <- data.frame("E" = c("O" = 10, "O_" = 14), "E_" = c("O" = 21, "O_" = 7))
d$n <- rowSums(d)
rbind(d, "n" = colSums(d))
14/21
7/21
10/25
15/25
16+8
16/24
10/24
9/21
2/5
1/5
4/20
d <- data.frame("E" = c("O" = 10, "O_" = 16), "E_" = c("O" = 15, "O_" = 4))
d$n <- rowSums(d)
rbind(d, "n" = colSums(d))
E_ <- c("O" = 10, "O_" = 16)
E <- c("O" = 15, "O_" = 4)
d <- data.frame(E, E_)
d$n <- rowSums(d)
rbind(d, "n" = colSums(d))
E <- c("O" = 10, "O_" = 16)
E_ <- c("O" = 15, "O_" = 4)
d <- data.frame(E, E_)
d$n <- rowSums(d)
rbind(d, "n" = colSums(d))
16/20
0.5/6
0.5/3
library(dplyr)
## read the math class questionaire data
math <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_3140/datasets/student-mat.csv",sep=";",header=TRUE)
## read the portuguese class questionaire data
por <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_3140/datasets/student-por.csv",sep=";",header=TRUE)
## common columns to merge the datasets by
join_by <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
## join the two datasets by selected common variables
math_por <- inner_join(math, por, by = join_by, suffix = c(".math", ".por"))
## which columns in the datasets were not used for joining the data
notjoined_columns <- colnames(math)[!colnames(math) %in% join_by]
## create a new data frame with the common columns
alc <- select(math_por, one_of(join_by))
## average / combine the rest of the columns
for(column_name in notjoined_columns) {
df <- select(math_por, starts_with(column_name))
if(is.numeric(select(df, 1))) {
alc[column_name] <- rowMeans(df)
} else {
alc[column_name] <- select(df, 1)
}
}
## combine weekday and weekend alcohol use into alc_use
alc <- mutate(alc, alc_use = (Dalc + Walc) / 2)
## transform alc_use into a binary (T, F) variable high_use
alc <- mutate(alc, high_use = alc_use > 2)
path <- "../datasets/alc.txt"
write.table(file = path, alc, sep = ",", row.names = F)
# str(read.table(path, sep=",", header = T))
setwd("C:/Users/Tuomo/Dropbox/GitHub/Helsinki-Open-Data-Science/datasets")
library(dplyr)
## read the math class questionaire data
math <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_3140/datasets/student-mat.csv",sep=";",header=TRUE)
## read the portuguese class questionaire data
por <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_3140/datasets/student-por.csv",sep=";",header=TRUE)
## common columns to merge the datasets by
join_by <- c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet")
## join the two datasets by selected common variables
math_por <- inner_join(math, por, by = join_by, suffix = c(".math", ".por"))
## which columns in the datasets were not used for joining the data
notjoined_columns <- colnames(math)[!colnames(math) %in% join_by]
## create a new data frame with the common columns
alc <- select(math_por, one_of(join_by))
## average / combine the rest of the columns
for(column_name in notjoined_columns) {
df <- select(math_por, starts_with(column_name))
if(is.numeric(select(df, 1))) {
alc[column_name] <- rowMeans(df)
} else {
alc[column_name] <- select(df, 1)
}
}
## combine weekday and weekend alcohol use into alc_use
alc <- mutate(alc, alc_use = (Dalc + Walc) / 2)
## transform alc_use into a binary (T, F) variable high_use
alc <- mutate(alc, high_use = alc_use > 2)
path <- "../datasets/alc.txt"
write.table(file = path, alc, sep = ",", row.names = F)
# str(read.table(path, sep=",", header = T))
