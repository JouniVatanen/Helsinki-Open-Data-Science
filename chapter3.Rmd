---
title       : Clustering and classification
description : Datasets in R, Linear Discriminant Analysis (LDA) and K-means clustering

--- type:NormalExercise lang:r xp:100 skills:1 key:3db2d48f25
## Datasets inside R

Welcome to the *Clustering and classification* chapter.

R has many (usually small) datasets already loaded in. There are also datasets included in the package installations. Some of the datasets are quite famous (like the [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) flower data) and they are frequently used for teaching purposes or to demonstrate statistical methods. 

This week we will be using the [Boston](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) dataset from the MASS package. Let's see how it looks like!

*** =instructions
- Load the `Boston` dataset from MASS
- Explore the `Boston` dataset. Look at the structure with `str()` and use `summary()` to see the details of the variables.
- Draw the plot matrix with `pairs()`

*** =hint
- You can draw the `pairs()` plot by typing the object name where it is saved.

*** =pre_exercise_code
```{r}

```

*** =sample_code
```{r}
# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset



# plot matrix of the variables


```

*** =solution
```{r}
# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
summary(Boston)

# plot matrix of the variables
pairs(Boston)

```

*** =sct
```{r}
test_output_contains("summary(Boston)", incorrect_msg = "Did you use the summary() function on the Boston data?")
test_output_contains("str(Boston)", incorrect_msg = "Did you use the str() function on the Boston data?")
test_function("pairs", incorrect_msg = "Did you draw the plot matrix with pairs?")

test_error()
success_msg("Good work!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:3a4eacf66c
## Scale the whole dataset

Usually the R datasets do not need much data wrangling as they are already in a good shape. But we will need to do little adjustments.

For later use, we will need to scale the data. In the scaling we subtract the column means from the corredponding columns and divide the difference with standard deviation.

$ scaled_x = \frac{x - mean(x)}{ sd(x)}$

The Boston data contains only numerical values, so we can use the function `scale()` to standardize the whole dataset.

*** =instructions
- Use the `scale()` function on the `Boston` dataset. Save the scaled data to `boston_scaled` object.
- Use `summary()` to look at the scaled variables. Note the means of the variables.
- Find out the class of the scaled object by executing the `class()` function. 
- Later we will want the data to be a data frame. Use `as.data.frame()` to convert the `boston_scaled` to a data frame format. Keep the object name as `boston_scaled`.

*** =hint
- `?scale`

*** =pre_exercise_code
```{r}
library(MASS)
data("Boston")
```

*** =sample_code
```{r}
# MASS and Boston dataset are available

# center and standardize variables
boston_scaled <- "change me!"

# summaries of the scaled variables


# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame


```

*** =solution
```{r}
# MASS and Boston dataset are available

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```

*** =sct
```{r}
test_function("scale", args = "x",incorrect_msg = "Did you scale the dataset?")

test_function("as.data.frame", args = "x",incorrect_msg = "Please use the function as.data.frame() to convert the data to a data frame.")

test_function("summary", args = "object", incorrect_msg = "Please use the function summary() to look at the summaries of the data variables.")

test_error()
success_msg("Good work!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:3d45bd1da6
## Creating a factor variable

We can create a categorical variable from a continuous one. There are many ways to to do that. Let's choose the variable `crim` (per capita crime rate by town) to be our factor variable. The description of the Boston dataset variables could be seen [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html). We want to cut the variable by [quantiles](https://en.wikipedia.org/wiki/Quantile) to get the high, low and middle rates of crime into their own categories.

See how it's done below!

*** =instructions
- Look at the summary of the scaled variable `crim`
- Use the function `quantile()` on the scaled crime rate variable and save the results to `bins`. Print the results.
- Create categorical crime vector with the `cut()` function. Set the `breaks` argument to be the quantile vector you just created.
- Use the function `table()` on the `crime` object
- Adjust the code of `cut()` by adding the `label` argument in the function. Create a string vector with the values `"low"`, `"med_low"`, `"med_high"`, `"high"` (in that order) and use it to set the labels.
- Do the table of the `crime` object again
- Execute the last lines of code to remove the original crime rate variable and adding the new one to scaled Boston dataset. 

*** =hint
- 

*** =pre_exercise_code
```{r}
library(MASS)
data("Boston")
boston_scaled <- as.data.frame(scale(Boston))
```

*** =sample_code
```{r}
# MASS, Boston and boston_scaled are available

# summary of the scaled crime rate


# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = "change me!", include.lowest = TRUE)

# look at the table of the new factor crime


# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

*** =solution
```{r}
# MASS, Boston and boston_scaled are available

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

*** =sct
```{r}
test_output_contains("summary(boston_scaled$crim)", incorrect_msg = "Did you use the summary() function on the crim variable in boston_scaled data?")

test_function("cut", args = "labels", incorrect_msg = "Did you set the labels to cut()?")
test_object("bins", incorrect_msg = "Did you create `bins` ?")

test_error()
success_msg("Nicely done! By the way, sometimes there are functions with the same name in different R packages (like `select` in **dplyr** and **MASS**). You can choose which one to use by adding the name of the package with two colons in front of the function call (for example `dplyr::select()`).")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:bca5932717
## Splitting the data to train and test sets

Exercise info here

*** =instructions
- Use the function `nrow()` on the `boston_scaled` to get the number of rows in the dataset. Save the number of rows in `n`.
- Execute the code to choose randomly 80% of the rows and save the row numbers to `ind`
- Create `train` set by selecting the row numbers that are saved in `ind`.
- Create `test` set by subtracting the rows that are used in the train set
- Take the crime classes from the `test` and save them as `correct_classes`
- Execute the code to remove `crime` from `test` set

*** =hint
- hint

*** =pre_exercise_code
```{r}
library(MASS)
boston_scaled <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/boston_scaled.txt", sep  =",", header = T)
```

*** =sample_code
```{r}
# number of rows in the Boston dataset 
n <- "change me!"

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- "change me!"

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

*** =solution
```{r}
# boston_scaled is available

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

```

*** =sct
```{r}
test_object("n", incorrect_msg = "Did you create n with nrow()?")
test_object("correct_classes", incorrect_msg = "Did you create `correct_classes`?")
test_error()
success_msg("Splitting data is heavy work, well done!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:2de2d66d11
## Linear Discriminant analysis

[Linear Discriminant analysis](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) is a classification (and dimension reduction) method. It finds the (linear) combination of the variables that separate the target variable classes. The target can be binary or multiclass variable. 

Linear discriminant analysis is closely related to many other methods, such as principal component analysis (we will look into that next week) and the already familiar logistic regression. 

LDA can be visualized with a biplot. We will talk more about biplots next week. The LDA biplot arrow function used in the exercise is (with slight changes) taken from [this](http://stackoverflow.com/questions/17232251/how-can-i-plot-a-biplot-for-lda-in-r) Stack Overflow message thread.

*** =instructions
- Fit a linear discriminant analysis with the function `lda()`. The function takes a formula (like in regression) as a first argument. Use the `crime` as a target variable and all the other variables as predictors. Hint! You can type `target ~ .` where the dot means all other variables in the data.
- Print the `lda.fit` object
- Create a numeric vector of the target classes (for plotting purposes)
- Use the function `plot()` on the `lda.fit` model
- Adjust the code: add arguments `col = classes` and `pch = classes` to the plot.
- Execute the `lda.arrow()` function (if you haven't done that already). Draw the plot with the lda arrows. Note that in DataCamp you will need to select both lines of code and execute them at the same time for the `lda.arrow()` function to work.
- Print out the linear combination coefficients (scaling) that LDA produced for every variable. If the target variable has $n$ categories, then LDA will produce $n-1$ linear discriminants (LD1, LD2, ... ).

*** =hint
- hint

*** =pre_exercise_code
```{r}
library(MASS)
boston_scaled <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/boston_scaled.txt", sep  =",", header = T)

ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)

```

*** =sample_code
```{r}
# MASS and boston_scaled are available

# linear discriminant analysis
lda.fit <- lda(" change me!", data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot("change me!", dimen = 2)
lda.arrows(lda.fit, myscale = 1)

```

*** =solution
```{r}
# MASS and boston_scaled are available

# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

*** =sct
```{r}
test_function("plot", incorrect_msg = "Did you plot the lda.fit object?")
test_function("plot", args= "col", incorrect_msg = "Did you set the color in the plot?")
test_error()
success_msg("Expanding your statistcal skills! Nice!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:0efb23a544
## Predict LDA

Like in the regression, the function `predict()` can be used to predict values based on a model. The function arguments are almost the same. You can see the help page of prediction function for LDA with `?predict.lda`.

Splitting the original data to test and train sets allows us to check how well our model works. So the training of the model is done with the train set and model validation is done with the test set.  


*** =instructions
- Save the correct crime classes of the test data into new object `correct_classes`
- Remove the `crime` variable from the test data
- Predict the crime classes with the test data
- Create a table of the results with `table()`, where you compare the correct classes to the one that model produced. You can get the predicted classes with `lda.pred$class`. 
- Did the classifier predict the crime rate correctly?

*** =hint
- hint

*** =pre_exercise_code
```{r}
library(MASS)
boston_scaled <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/boston_scaled.txt", sep  =",", header = T)

# create train and test set 
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]

# linear discriminant analysis
lda.fit = lda(crime ~ ., data=train)

```

*** =sample_code
```{r}
# lda.fit, train and test are available

# save the correct classes from test data
correct_classes <- "change me!"

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = "change me!")

# cross tabulate the results


```

*** =solution
```{r}
# lda.fit, train and test are available


# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct_classes, lda.pred$class)

```

*** =sct
```{r}
test_function("table", incorrect_msg = "Did you create a table of the predicted classes and the correct ones?")


test_error()
success_msg("Good work!")
```


--- type:NormalExercise lang:r xp:100 skills:1 key:50338b95ba
## LDA biplot

The loadings that linear discriminant model produces are coefficients for the variables used in the model creation. These coefficients can be used as arrows in biplot. From biplot you can quickly see which variables have the most impact on the linear discriminants. 



*** =instructions
- Create `classes` and `predictions`. The `crime` variable is removed from the train set, because otherwise the [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) won't work.
- Create the `matrix_product`. The matrix multiplication can be done with `%*%` in R.
- The class of `matrix_product` is matrix. Change it to data frame.

- Change the color in `lda.arrow()` function from `"red"` to `"steel blue"`. Also change the `myscale` argument to two. Draw the plot again.

*** =hint
- hint

*** =pre_exercise_code
```{r}
library(MASS)
boston_scaled <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/boston_scaled.txt", sep  =",", header = T)

# create train and test set 
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]

# linear discriminant analysis
lda.fit = lda(crime ~ ., data = train)
```

*** =sample_code
```{r}
# train and lda.fit are available

# crime classes as numeric
classes <- as.numeric(train$crime)

# remove the crime variable
predictors <- dplyr::select(train, -crime)

# matrix product of the predictors and linear discriminant loadings
matrix_product <- as.matrix(predictors) %*% lda.fit$scaling

# from matrix to data frame
matrix_product <- as.data.frame(matrix_product)



# plot the results with arrow loadings 
plot(matrix_product$LD1, matrix_product$LD2, col=classes)
lda.arrows(lda.fit, myscale = 1)

```

*** =solution
```{r}
# train and lda.fit are available

# crime classes as numeric
classes <- as.numeric(train$crime)

# remove the crime variable
predictors <- dplyr::select(train, -crime)

# matrix product of the predictors and linear discriminant loadings
matrix_product <- as.matrix(predictors) %*% lda.fit$scaling

# from matrix to data frame
matrix_product <- as.data.frame(matrix_product)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "steel blue", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# plot the results with arrow loadings 
plot(matrix_product$LD1, matrix_product$LD2, col=classes)
lda.arrows(lda.fit, myscale = 2)

```

*** =sct
```{r}
test_function("lda.arrows", args = "myscale",incorrect_msg = "Did you set myscale to two in the lda.arrows() function?")

test_error()
success_msg("Good work!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:10414fc9d1
## More dimensions with Plotly!

[Plotly](https://plot.ly/r/) is an R graphing library that creates **interactive** plots and has an ggplot2 integration as well. Let's try it out and visualize LDA a bit more!

*** =instructions
- Access plotly
- Execute the code for the `plot_ly()` plot
- Adjust the code: add argument `color` as a function argument. Set the color to be the crime classes of the train set.


*** =hint
- You can access the crime classes of the train data with `train$crime`

*** =pre_exercise_code
```{r}
library(MASS)
boston_scaled <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/boston_scaled.txt", sep  =",", header = T)

n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]

lda.fit = lda(crime ~ ., data=train)
predictors <- dplyr::select(train, -crime)
matrix_product <- as.matrix(predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

```

*** =sample_code
```{r}
# train, lda.fit and matrix_product are available

# access plotly
library(plotly)

# plotly plot
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')

```

*** =solution
```{r}
# train, lda.fit and matrix_product are available

# access plotly
library(plotly)

# plotly plot
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type =  'scatter3d', mode = 'markers', color = train$crime)

```

*** =sct
```{r}
test_function("plot_ly", args = "color",incorrect_msg = "Did you set the color correctly in plot_ly()?")
test_error()
success_msg("Plot_ting like no tomorrow!")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:6719c6079e
## Towards clustering: distance measures

Similarity or dissimilarity of objects can be measured with distance measures. There are many different measures for different types of data. The most common or "normal" distance measure is [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). 

There are functions that calculate the distances in R. In this exercise, we will be using the base R's `dist()` function. The function creates a distance matrix that is saved as dist object. The distance matrix is usually square matrix containing the pairwise distances of the observations. So with large datasets, the computation of distance matrix is time consuming and storing the matrix might take a lot of memory. 

*** =instructions
- Load the MASS package and the `Boston` dataset from it
- Create `dist_eu` by calling the `dist()` function on the Boston dataset. Note that by default, the function uses Euclidean distance measure.
- Look at the summary of the `dist_eu`
- Next create object `dist_man` that contains the Manhattan distance matrix of the Boston dataset
- Look at the summary of the `dist_man`

*** =hint
- `data(*name_of_the_dataset*)` can be used to load dataset from R package
- Look at the help page of `dist()` function with `?dist`

*** =pre_exercise_code
```{r}
library(MASS)
data('Boston')
```

*** =sample_code
```{r}
# load MASS and Boston
library(MASS)
data('Boston')

# euclidean distance matrix
dist_eu <- "change me!"

# look at the summary of the distances


# manhattan distance matrix
dist_man <- "change me!"

# look at the summary of the distances


```

*** =solution
```{r}
# load MASS and Boston
library(MASS)
data('Boston')

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)

```

*** =sct
```{r}
test_object("dist_man", incorrect_msg = "Did you create dist_man?")
test_output_contains("summary(dist_eu)", incorrect_msg = "Did you use the summary() function on the dist_eu?")
test_output_contains("summary(dist_man)", incorrect_msg = "Did you use the summary() function on the dist_man?")
test_error()
success_msg("Dissimilar might be similar if measured in a different way. Mind = blown!")
```


--- type:NormalExercise lang:r xp:100 skills:1 key:f3d6cd6c00
## K-means: determine the k

[K-means](https://en.wikipedia.org/wiki/K-means_clustering) is maybe the most used and known clustering method. It is an unsupervised method, that assigns observations to groups or **clusters** based on similarity of the objects.

K-means needs the number of clusters as an argument. One way to determine the number of clusters is to look at how the within cluster sum of squares behaves when the number of cluster changes. 

K-means might produce different results every time, because it randomly assigns the initial cluster centers. The function `set.seed()` can be used to deal with that. 

*** =instructions
- Execute the code for the distance matrix
- Set the max number of clusters (`k_max`) to be 10.
- Execute the code to calculate within sum of squares. This might take a while.
- Visualize the within sum of squares when the number of cluster goes from 1 to 10. The optimal number of clusters is when the value of within sum of squares changes radically. 

*** =hint
- Simply adjust the part of the code that says "change me!"

*** =pre_exercise_code
```{r}
library(MASS)
library(ggplot2)

data('Boston')

```

*** =sample_code
```{r}
# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# euclidean distance matrix
dist_eu <- dist(Boston)

# determine the number of clusters
k_max <- "change me!"

# calculate within sum of square
wss <- sapply(1:k_max, function(k){kmeans(dist_eu, k, nstart=10)$tot.withinss})

# visualize WSS
qplot(x = 1:k_max, y = wss, geom = 'line', main = 'Elbow method')

```

*** =solution
```{r}
# MASS, ggplot2 and Boston dataset are available

# euclidean distance matrix
dist_eu <- dist(Boston)

# determine the number of clusters
k_max <- 10

# calculate within sum of square
wss <- sapply(1:k_max, function(k){kmeans(dist_eu, k, nstart=10)$tot.withinss})

# visualize WSS
qplot(x = 1:k_max, y = wss, geom = 'line', main = 'Elbow method')

```

*** =sct
```{r}

# test_function("", args = "",incorrect_msg = "")
test_object("k_max", incorrect_msg = "Did you set the k_max to 10?")
# test_output_contains("", incorrect_msg = "")
# 
# test_error()
success_msg("Well done! ")
```

--- type:NormalExercise lang:r xp:100 skills:1 key:63da6251d5
## K-means clustering

In the Elbow plot the optimal number of clusters is when the within cluster sum of squares drops (when there is an "elbow" in the curve). In this case, two clusters would seem optimal. Let's see how the clusters will look like. 

*** =instructions
- Based on the elbow plot we will want to have two clusters. 

*** =hint
- hint

*** =pre_exercise_code
```{r}
library(MASS)
library(ggplot2)

data('Boston')
set.seed(13)
dist_eu <- dist(Boston)
k_max <- 10
wss <- sapply(1:k_max, function(k){kmeans(dist_eu, k, nstart=10)$tot.withinss})
qplot(x = 1:k_max, y = wss, geom = 'line', main = 'Elbow method')
```

*** =sample_code
```{r}
# Boston and dist_eu are available

#k-means clustering
km <-kmeans(dist_eu, k = "change me!")

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

*** =solution
```{r}
# Boston and dist_eu are available

#k-means clustering
km <-kmeans(dist_eu, k = "change me!")

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

*** =sct
```{r}

# test_function("", args = "",incorrect_msg = "")
# test_object("", incorrect_msg = "")
# test_output_contains("", incorrect_msg = "")
# 
# test_error()
success_msg("Clustering like a pro!")
```
